{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ore Detector Proof of Concept.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Platinum-Dragon/Ore-detector-presentation/blob/main/Ore_Detector_Proof_of_Concept.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb9V6eUq-eyr"
      },
      "source": [
        "#Machine Learning Algorithm has Promise in Learning to Detect Ore Through Discoloration in Visible Light\n",
        "\n",
        "Satellite images are already used to identify potential sources of minerals and ore by scanning the images for signature light frequencies that are reflected from the terrain where the ore may be present.\n",
        "\n",
        "We trained a CNN to recognize terrain containing quartz deposits from satellite images with 66.5% (+/- 1.5%) accuracy.\n",
        "\n",
        "The idea for this project was inspired by this paper:\n",
        "https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2016GC006501\n",
        "where light signatures were being used to detect the presence of various mineral ores.\n",
        "\n",
        "Below is the procedure we used.\n",
        "First, we make sure we have some essential packages installed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQozL9uJscNf"
      },
      "source": [
        "!pip install pillow\n",
        "!pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmoVmwmBDjU-"
      },
      "source": [
        "Next, import some necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Farrh1g7skw-"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTkXJsSwjMAv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12d4717a-6248-4884-ab46-bd580ece73a1"
      },
      "source": [
        "print(f\"TPU address-> grpc://{os.environ['COLAB_TPU_ADDR']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address-> grpc://10.13.254.154:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnOeHFodDuPv"
      },
      "source": [
        "Let's mount our Google Drive so that we can store our data and share it if need be, and also define the path that we will store our satellite images in.\n",
        "\n",
        "This Colab will store a root path in order to know where to look for any files\n",
        "needed during operation.\n",
        "\n",
        "If your path is different than:\n",
        "\"/content/drive/My Drive/Colab Notebooks/Ore Detector/\"\n",
        "then you will have to modify it accordingly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KXgfeOAuiaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bffd0ff4-1f09-4e49-f324-de0ecb4e6a9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# This is the working directory for this Colab\n",
        "ROOT_PATH = \"/content/drive/My Drive/Colab Notebooks/Ore Detector/\"\n",
        "\n",
        "# provide link to download our image datset.\n",
        "\n",
        "# These global variables will allow you to change the directories that the \n",
        "# satellite images will be written to or the coordinate dataset will be read\n",
        "# from.\n",
        "SAT_IMG_DIR = \"SatImages/\"\n",
        "LAT_LONG_DIR = \"Lat Long Datasets/\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av8nw9miEE2n"
      },
      "source": [
        "Now, let's go get some satellite images. We already have a dataset of latitude and longitude coordinates of known mineral sources, some containing quartz and some not, stored in a csv file. So for each set of coordinates we need to fetch its corresponding image.\n",
        "\n",
        "To fetch each image, we will be using Google Static Maps API, and you will need an API key to use it.\n",
        "\n",
        "Here is how to get your API key:https://developers.google.com/maps/documentation/embed/get-api-key\n",
        "\n",
        "When you have your API key, place it in a folder in the Ore Detector directory called:\n",
        "\"map_apikey\" and the Colab will look for it there.\n",
        "\n",
        "A call to Google Static Maps API will give us our image. The data row already has a column indicating whether there is quartz or not, so we can save the image to the \"quartz\" directory if it has quartz, and the \"notquartz\" directory if not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_9ZIY-y1bv_"
      },
      "source": [
        "# TODO change the key to a constant, provide link explaining how they\n",
        "# can get their own key and then read key from local folder.\n",
        "\n",
        "'''\n",
        "This cell will fetch the satellite image for each of the coordinates listed\n",
        "in the dataset\n",
        "'''\n",
        "# Set this flag to true if the image data needs to be reconstructed again.\n",
        "BUILD_IMAGE_DATA = False\n",
        "\n",
        "if BUILD_IMAGE_DATA:\n",
        "  GOOGLE_STATIC_MAPS_ADDY = \"https://maps.googleapis.com/maps/api/staticmap?\"\n",
        "  dfMineral = pd.read_csv(f\"{ROOT_PATH}{LAT_LONG_DIR}MagnetiteLatLong.csv\")\n",
        "\n",
        "  # First get the API key for calling Google Maps\n",
        "  try:\n",
        "    with open(f\"{ROOT_PATH}map_apikey\", \"r\") as f:\n",
        "      MAPS_API_KEY = f.read()\n",
        "  except FileNotFoundError:\n",
        "    print(\"API key file not found\")\n",
        "    exit(-1)\n",
        "\n",
        "  # Parameter list for map image call\n",
        "  picSize = \"600x600\"\n",
        "  zoomLevel = 7\n",
        "  # mapType = satellite\n",
        "  # key = MAPS_API_KEY\n",
        "\n",
        "  # For each location in the data frame, obtain the satellite picture for\n",
        "  # that location and write it to the appropriate directory.\n",
        "  for i in range(0, dfMineral.shape[0]):\n",
        "    # Construct the URL for the image at these coordinates\n",
        "    url = GOOGLE_STATIC_MAPS_ADDY + \\\n",
        "          f\"center={dfMineral.loc[i, 'latitude']},{dfMineral.loc[i, 'longitude']}\" + \\\n",
        "          f\"&size={picSize}\" + \\\n",
        "          f\"&zoom={zoomLevel}\" + \\\n",
        "          \"&maptype=satellite\" + \\\n",
        "          f\"&key={MAPS_API_KEY}\"\n",
        "\n",
        "    # Request and save one picture at this URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "    # If the fetch is successful, save the image in the appropriate directory\n",
        "\n",
        "      if dfMineral.loc[i, 'magnetite']:\n",
        "        path = ROOT_PATH + SAT_IMG_DIR + f\"mineral/mg{i}.png\"\n",
        "      else:\n",
        "        path = ROOT_PATH + SAT_IMG_DIR + f\"nomineral/nmg{i}.png\"\n",
        "\n",
        "      with open(path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq2S18l1FPBk"
      },
      "source": [
        "Now that we have our images, we need to build our actual training and validation data. First, we will import these Pytorch libraries because we will need the transform function to resize our pictures and keep them in a format that Pytorch will be able to work with. We also import the necessary modules for building and training the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCz2enHvwnE0"
      },
      "source": [
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnAaw3AkPDcV"
      },
      "source": [
        "The training data for this data set is imbalanced; there are many more negatives than positives for each type of mineral. To balance this out, we select and equal number of positive and negative cases to train on. Otherwise, the model would predict all observations as negative to come out with a better accuracy score, sacrificing recall in the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zeUqTJ2dGpy"
      },
      "source": [
        "def equal_pos_neg (training_data):\n",
        "  \"\"\"\n",
        "  Positive cases must be < negatives to call this function\n",
        "  \"\"\"\n",
        "  i = 0\n",
        "\n",
        "  while (training_data[i][1] == training_data[0][1]):\n",
        "    i += 1\n",
        "\n",
        "  begin_positives_idx = i\n",
        "  number_of_positives = len(training_data) - i\n",
        "  print(f\"Number of negative cases: {i} number of positive cases {number_of_positives}\")\n",
        "  negatives, positives = training_data[0:number_of_positives], training_data[begin_positives_idx: -1]\n",
        "                                                            \n",
        "  print(f\"Length of negatives: {len(negatives)}\")\n",
        "  print(f\"Positives: {len(positives)}\")\n",
        "  return negatives + positives"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnMh9NAVF5gm"
      },
      "source": [
        "Here, we set up some constants including a resize function. We will read each of the images in the quartz folder, resize it to the desired dimensions and append it to our list of training data. We will do the same with the notquartz pictures so that we have our labeled training data. Finally, the data is saved so that we can load it in when we are ready to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPDrTZaT2USC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409dc252-ce2f-4ac1-caa1-23d92617a279"
      },
      "source": [
        "'''\n",
        "This cell will take the images in the quartz or notquartz directories,\n",
        "resize them, and create a list of images with their respective labels\n",
        "for use in training and validation.\n",
        "'''\n",
        "MAKE_TRAINING_DATA = False\n",
        "\n",
        "class MineralOrNot ():\n",
        "  MINERAL = ROOT_PATH + SAT_IMG_DIR + \"quartz\"\n",
        "  NOT_MINERAL = ROOT_PATH + SAT_IMG_DIR + \"notquartz\"\n",
        "  LABELS = {NOT_MINERAL: 0, MINERAL: 1}\n",
        "  RESIZE_SIZE = 100\n",
        "  trainingData = []\n",
        "  resize = T.Resize(RESIZE_SIZE, interpolation=Image.BICUBIC)\n",
        "  # resize = T.Compose([T.Resize(RESIZE_SIZE, interpolation=Image.CUBIC)])\n",
        "\n",
        "  def makeTrainingData(self):\n",
        "    for label in self.LABELS:\n",
        "      for f in os.listdir(label):\n",
        "        imgPath = os.path.join(label, f)\n",
        "        img = Image.open(imgPath).convert(\"RGB\")\n",
        "        # image will be resized here\n",
        "        im = self.resize(img)\n",
        "        # Make labels a single vector 1 = mineral, 0 = no mineral\n",
        "        self.trainingData.append([np.array(im), self.LABELS[label]])\n",
        "        '''\n",
        "        Create 3 more versions of the image each rotated 90 degrees to\n",
        "        increase the tranining data:\n",
        "        '''\n",
        "        for i in range(3):\n",
        "          im = TF.rotate(im, 90)\n",
        "          self.trainingData.append([np.array(im), self.LABELS[label]])\n",
        "\n",
        "        # The line below was to one-hot encode the labels, but the layer\n",
        "        # is expecting a 1D vector. \n",
        "        # self.trainingData.append([np.array(im), np.eye(2)[self.LABELS[label]]])\n",
        "\n",
        "    # Grab an equal number of positive and negative training cases\n",
        "    self.trainingData = equal_pos_neg(self.trainingData)\n",
        "\n",
        "    np.random.shuffle(self.trainingData)\n",
        "    np.save(f\"{ROOT_PATH}training_data.npy\",\n",
        "            self.trainingData)\n",
        "\n",
        "if MAKE_TRAINING_DATA:\n",
        "  allData = MineralOrNot()\n",
        "  allData.makeTrainingData()\n",
        "  print(f\"Training data length: {len(allData.trainingData)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of negative cases: 1592 number of positive cases 716\n",
            "Length of negatives: 716\n",
            "Positives: 715\n",
            "Training data length: 1431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTVrgpBoCJHK"
      },
      "source": [
        "#Load the Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M52jFGYzkxdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d73e55-c1f8-4473-cce2-0b19c9e6155b"
      },
      "source": [
        "#Simply loading our labeled dataset into this variable\n",
        "trainTestDataset = np.load(f\"{ROOT_PATH}training_data.npy\",\n",
        "                           allow_pickle=True)\n",
        "\n",
        "print(len(trainTestDataset))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-e2U_BqDGi1"
      },
      "source": [
        "The data is divided into 90% training and 10% validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDDk_htnxDYK"
      },
      "source": [
        "# Train test split\n",
        "X = [x[0] for x in trainTestDataset]\n",
        "y = [y[1] for y in trainTestDataset]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, stratify=y, random_state=123)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jnDj6Oq3Fzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30361d8c-a1bf-4f63-a79e-db7c0bdb272a"
      },
      "source": [
        "print(f\"Shape of X train element: {X_train[0].shape}\")\n",
        "print(f\"First y train element: {y_train[0]}\")\n",
        "print(f\"Len of X train: {len(X_train)}\")\n",
        "print(f\"Len X val: {len(X_val)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X train element: (100, 100, 3)\n",
            "First y train element: 1\n",
            "Len of X train: 1287\n",
            "Len X val: 144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nm7Nh3_DZgz"
      },
      "source": [
        "# Build the Model\n",
        "A 2-layer CNN with 3x3 kernels is built using Pytorch functions. This very simple setup was chosen so that we could quickly get an indication if our idea was on track."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZsYlvJn36id"
      },
      "source": [
        "class CNNMine(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(CNNMine, self).__init__()\n",
        "      linear_input_size = 1875\n",
        "      self.cnn_layers = nn.Sequential(\n",
        "        # Defining FIRST 2D convolution layer\n",
        "        nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(3),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        # Defining SECOND 2D convolution layer\n",
        "        nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(3),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "      )\n",
        "\n",
        "      self.linear_layers = nn.Sequential(\n",
        "          nn.Linear(linear_input_size, 2)\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.cnn_layers(x)\n",
        "      x = x.view(x.size(0), -1)\n",
        "      x = self.linear_layers(x)\n",
        "      return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TPzDM-TDhng"
      },
      "source": [
        "This function will train the model for the desired number of epochs. 100 epochs were performed in order to see if/when the training and validation data converged. The Adam optimizer was chosen here because it was the quickest and simplest to implement for our discovery purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlADV0uV4bZh"
      },
      "source": [
        "def train_model(epochs, model):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "      optimizer = Adam(model.parameters(), lr=0.0125)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      model.train()\n",
        "      tr_loss = 0\n",
        "      ptX_train = Variable(torch.tensor(X_train)).float()\n",
        "      # The model is expecting Batch, Channel, Width, Height so our\n",
        "      # tensor needs to be rearranged\n",
        "      ptX_train = ptX_train.permute(0, 3, 1, 2)\n",
        "      pty_train = Variable(torch.tensor(y_train)).float()\n",
        "      ptX_val = Variable(torch.tensor(X_val)).float()\n",
        "      # The model is expecting Batch, Channel, Height, Width so\n",
        "      # rearrange here also\n",
        "      ptX_val = ptX_val.permute(0, 3, 1, 2)\n",
        "      pty_val = Variable(torch.tensor(y_val)).float()\n",
        "      optimizer.zero_grad()\n",
        "      output_train = model(ptX_train)\n",
        "      output_val = model(ptX_val)\n",
        "      loss_train = criterion(output_train, pty_train.squeeze().long())\n",
        "      loss_val = criterion(output_val, pty_val.squeeze().long())\n",
        "      train_losses.append(loss_train)\n",
        "      val_losses.append(loss_val)\n",
        "      loss_train.backward()\n",
        "      optimizer.step()\n",
        "      tr_loss = loss_train.item()\n",
        "      if epoch%5 == 0:\n",
        "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)\n",
        "      # plotting the training and validation loss\n",
        "    start_time = time.time()\n",
        "    print('Predicting training set')\n",
        "    predict_set(ptX_train, pty_train, model)\n",
        "    end_time = time.time()\n",
        "    print('Prediction of ' + str(len(ptX_train)) + ' samples took ' + str(end_time - start_time) + ' seconds')\n",
        "    print('Predicting validation set')\n",
        "    predict_set(ptX_val, pty_val, model) \n",
        "    # test_x = Variable(torch.tensor(model.test_x).unsqueeze(1)).float()\n",
        "    # test_y = Variable(torch.tensor(model.test_y).unsqueeze(1)).float()\n",
        "    # print('Predicting test set')\n",
        "    # predict_set(test_x, test_y, model)\n",
        "    torch.save(model.state_dict(), f\"{ROOT_PATH}oremodel.pt\")\n",
        "    plt.figure(figsize=[15,12])\n",
        "    plt.plot(train_losses, label='Training loss')\n",
        "    plt.plot(val_losses, label='Validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMThVSsDE4hR"
      },
      "source": [
        "These functions are used to call the training function and to asses the accuracy of the model after the last training epoch has been completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IMsZGs97e8c"
      },
      "source": [
        "def train_ore_detector(model = None, epochs=100, w = 800, h = 600):\n",
        "  if model == None:\n",
        "    model = CNNMine()\n",
        "  # model.load_training_data()\n",
        "  train_model(epochs, model)\n",
        "\n",
        "def predict_set(set_values, labels, model):\n",
        "    predictions = has_ore(set_values, model)\n",
        "    print(str(accuracy_score(labels, predictions)*100) + '% accuracy on target dataset')\n",
        "\n",
        "    # Let's only look at the validation results for now.\n",
        "    print(f\"Predictions length: {len(predictions)}\")\n",
        "    results_df = pd.DataFrame(data=zip(labels, predictions), columns=['labels', 'predictions'])\n",
        "\n",
        "    print(f\"Confusion matrix:\\n {confusion_matrix(labels, predictions)}\")\n",
        "\n",
        "    if len(predictions) < 100:  # Only print the validation set\n",
        "      pd.set_option('display.max_rows', 200)\n",
        "      print(results_df.head(200))\n",
        "\n",
        "def has_ore(value, model):\n",
        "    with torch.no_grad():\n",
        "        output = model(value)\n",
        "    softmax = torch.exp(output)\n",
        "    prob = list(softmax.numpy())\n",
        "    return np.argmax(prob, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Sv6MaYDxPq"
      },
      "source": [
        "#Train the Detector\n",
        "\n",
        "Below, we show the output of one training run. The blue line represents the loss on the training phase, while the orange line represents the loss validation phase. We can see that they gradually decrease together, finally achieving their minimums.\n",
        "\n",
        "The values are close together and do converge around 100 epochs, suggesting that 100 epochs is sufficient to train this model on this dataset.\n",
        "\n",
        "This training step was run multiple times and the average was taken in order to get a figure for the model's accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3HPbP8L71Pg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "30308bbc-7695-47fc-ba19-4b665f7ad045"
      },
      "source": [
        "train_ore_detector()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8efaf177daf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ore_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-a97786a08911>\u001b[0m in \u001b[0;36mtrain_ore_detector\u001b[0;34m(model, epochs, w, h)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_ore_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNMine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;31m# model.load_training_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-67c4c8043c62>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       self.linear_layers = nn.Sequential(\n\u001b[0;32m---> 19\u001b[0;31m           \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m           \u001b[0;31m# nn.Linear(linear_input_size, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       )\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'out_features'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb-QcrweFi4G"
      },
      "source": [
        "Our goal was to achieve > 60% accuracy to determine if we should move forward with this project. After many attempts adjusting learning rates, number of layers as well as trying dropout layers, only a 50% accuracy was acheived which is not enough to consider this idea worth pursuing in its present form.\n",
        "\n",
        "Possible ideas to improve the concept:\n",
        "\n",
        "- More detailed photographs. The photos selected were selected at an arbitrary distance from the Earth's surface, perhaps closer, more detailed photos would provide more information/features for the algorithm to process.\n",
        "\n",
        "- Using more wavelengths of light. Mineral deposits can be found from photographs using wavelengs of light in or near the infrared spectrum. Including many bands beyond visible light may increase the ability of an algorithm to recognize patterns, but would likely increase the complexity and training time dramatically.\n"
      ]
    }
  ]
}